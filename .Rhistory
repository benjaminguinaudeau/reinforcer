actions == 1 & state %% 3 == 0 ~ 1,
actions == 1 & state %% 3 != 0 ~ -1,
actions == 0 & state %% 3 != 0 ~ 1,
actions == 0 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
reset <- function(){
list(
state = 0, # Initial state of the next episode
)
}
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
env_train$step(actions = 0)
env_train$reset()
list(
state = 0 # Initial state of the next episode
)
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
env_train$step(actions = 0)
env_train$row
env_train$reset()
env_train$step(actions = 0)
env_train$row
env_train$reset()
env_train$step(actions = 0)
env_train$row
env_train$reset()
env_train$step(actions = 0)
env_train$row
env_train$reset()
env_train$step(actions = 0)
env_train$row
env_train$reset()
env_train$step(actions = 0)
env_train$row
env_train$reset()
env_train$step(actions = 0)
env_train$row
env_train$reset()
env_train$step(actions = 0)
sb <- import("stablebaselines3")
sb <- import("stable-baselines3")
reticulate::repl_python()
sb <- import("stable_baselines3")
model_a2c <- sb$A2C'MlpPolicy', env_train, verbose=0)
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 10000)
model_a2c$learn(total_timesteps = 10)
model_a2c <- sb$a2c('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 10)
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 10)
model_a2c$learn(total_timesteps = 10L)
model_a2c$learn(total_timesteps = 10L)
env_train$step(1)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$step(0)
env_train$reset()
env_train$reset()
env_train$reset()
env_train$reset()
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
env_train$reset()
class(env_train$reset())
reticulate::repl_python()
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100L)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$reset()
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
step <- function(state, actions, step_index){
print(actions)
# Reward
reward <- dplyr::case_when(
actions == 1 & state %% 3 == 0 ~ 1,
actions == 1 & state %% 3 != 0 ~ -1,
actions == 0 & state %% 3 != 0 ~ 1,
actions == 0 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
env_train$step(actions = 1)
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100L)
reticulate::repl_python()
reticulate::source_python('~/gdrive/Konstanz/SideProjects/package/reinforcer/inst/gymr.py')
init <- function(){
list(
state = 0, # Default State
dim_state = c(1L, 1L), # Dimension of the environment state space
action_len = 1L, #Length of action vector
low = 0, # Lower  boundary of the action space
high = 1 # Upper  boundary of the action space
)
}
step <- function(state, actions, step_index){
# Reward
reward <- dplyr::case_when(
actions == 1 & state %% 3 == 0 ~ 1,
actions == 1 & state %% 3 != 0 ~ -1,
actions == 0 & state %% 3 != 0 ~ 1,
actions == 0 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
sb <- import("stable_baselines3")
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100L)
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100L)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
env_train$step(actions = 1)
a = NA
reticulate::repl_python()
a
a = NA_real_
reticulate::repl_python()
reticulate::source_python('~/gdrive/Konstanz/SideProjects/package/reinforcer/inst/gymr.py')
env_train$step(actions = 1)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
env_train$step(actions = NA)
actions <- NA
state
# Reward
reward <- dplyr::case_when(
actions == 1 & state %% 3 == 0 ~ 1,
actions == 1 & state %% 3 != 0 ~ -1,
actions == 0 & state %% 3 != 0 ~ 1,
actions == 0 & state %% 3 == 0 ~ -1
)
reward
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
list(state = state, reward = reward, terminal = terminal)
terminal <- F
list(state = state, reward = reward, terminal = terminal)
out <- list(state = state, reward = reward, terminal = terminal)
reticulate::repl_python()
step <- function(state, actions, step_index){
# Reward
reward <- dplyr::case_when(
actions == 1 & state %% 3 == 0 ~ 1,
actions == 1 & state %% 3 != 0 ~ -1,
actions == 0 & state %% 3 != 0 ~ 1,
actions == 0 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
reticulate::source_python('~/gdrive/Konstanz/SideProjects/package/reinforcer/inst/gymr.py')
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
env_train$step(actions = NA)
env_train$step(actions = NA_real_)
env_train$step(actions = NA_character_)
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100L)
step <- function(state, actions, step_index){
print(state)
print(actions)
# Reward
reward <- dplyr::case_when(
actions == 1 & state %% 3 == 0 ~ 1,
actions == 1 & state %% 3 != 0 ~ -1,
actions == 0 & state %% 3 != 0 ~ 1,
actions == 0 & state %% 3 == 0 ~ -1
)
print(reward)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100L)
step <- function(state, actions, step_index){
# Reward
reward <- dplyr::case_when(
actions > .5 & state %% 3 == 0 ~ 1,
actions > .5 & state %% 3 != 0 ~ -1,
actions <= .5 & state %% 3 != 0 ~ 1,
actions <= .5 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100L)
reticulate::source_python('~/gdrive/Konstanz/SideProjects/package/reinforcer/inst/gymr.py')
model_a2c$learn(total_timesteps = 10000L)
model_a2c$predict(state = 3)
model_a2c$predict(3)
model_a2c$predict(as.matrix(3))
model_a2c$predict(as.matrix(4))
model_a2c$predict(as.matrix(5))
model_a2c$predict(as.matrix(5), deterministic = T)
as.matrix(5)
model_a2c$predict(as.matrix(0), deterministic = T)
init <- function(){
list(
state = 0, # Default State
dim_state = c(1L, 1L), # Dimension of the environment state space
action_len = 1L, #Length of action vector
low = 0, # Lower  boundary of the action space
high = 1 # Upper  boundary of the action space
)
}
step <- function(state, actions, step_index){
# Reward
reward <- dplyr::case_when(
actions > .5 & state %% 3 == 0 ~ 1,
actions > .5 & state %% 3 != 0 ~ -1,
actions <= .5 & state %% 3 != 0 ~ 1,
actions <= .5 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
reticulate::use_condaenv("reinforcer", required = TRUE)
options(python_init = TRUE)
library(reticulate)
# library(reinforcer)
devtools::install()
# library(reinforcer)
devtools::install()
reticulate::use_condaenv("reinforcer", required = TRUE)
options(python_init = TRUE)
library(reticulate)
# library(reinforcer)
devtools::install()
devtools::document()
# library(reinforcer)
devtools::install()
# This loads some requird python dependencies
load_python_functions()
devtools::load_all()
# This loads some requird python dependencies
load_python_functions()
init <- function(){
list(
state = 0, # Default State
dim_state = c(1L, 1L), # Dimension of the environment state space
action_len = 1L, #Length of action vector
low = 0, # Lower  boundary of the action space
high = 1 # Upper  boundary of the action space
)
}
step <- function(state, actions, step_index){
# Reward
reward <- dplyr::case_when(
actions > .5 & state %% 3 == 0 ~ 1,
actions > .5 & state %% 3 != 0 ~ -1,
actions <= .5 & state %% 3 != 0 ~ 1,
actions <= .5 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 100
return(list(state = state, reward = reward, terminal = terminal))
}
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
env_train <- py$gymr(init = "init",
step = "step",
reset = "reset")
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
sb <- import("stable_baselines3")
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c$learn(total_timesteps = 100000L)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(0), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(sample(1:1000, 1)), deterministic = T)
model_a2c$predict(as.matrix(glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
model_a2c$predict(as.matrix(dplyr::glimpse(sample(1:1000, 1))), deterministic = T)
1:10 %>%
map_lgl(~{
model_a2c$predict(as.matrix(.x), deterministic = T)
})
apply(1:10, function(.x) model_a2c$predict(as.matrix(.x), deterministic = T))
apply(1:10, FUN = function(.x) model_a2c$predict(as.matrix(.x), deterministic = T))
tapply(1:10, FUN = function(.x) model_a2c$predict(as.matrix(.x), deterministic = T))
sapply(1:10, FUN = function(.x) model_a2c$predict(as.matrix(.x), deterministic = T))
sapply(1:1000, FUN = function(.x) model_a2c$predict(as.matrix(.x), deterministic = T))
sapply(1:1000, FUN = function(.x) model_a2c$predict(as.matrix(.x), deterministic = T))
1:1000 %% 3
state <- 1:1000
actions <- 1
dplyr::case_when(
actions > .5 & state %% 3 == 0 ~ 1,
actions > .5 & state %% 3 != 0 ~ -1,
actions <= .5 & state %% 3 != 0 ~ 1,
actions <= .5 & state %% 3 == 0 ~ -1
)
actions <- 0
dplyr::case_when(
actions > .5 & state %% 3 == 0 ~ 1,
actions > .5 & state %% 3 != 0 ~ -1,
actions <= .5 & state %% 3 != 0 ~ 1,
actions <= .5 & state %% 3 == 0 ~ -1
)
init <- function(){
list(
state = 0, # Default State
dim_state = c(1L, 1L), # Dimension of the environment state space
action_len = 1L, #Length of action vector
low = 0, # Lower  boundary of the action space
high = 1 # Upper  boundary of the action space
)
}
step <- function(state, actions, step_index){
# Reward
reward <- dplyr::case_when(
actions > .5 & state %% 3 == 0 ~ 1,
actions > .5 & state %% 3 != 0 ~ -1,
actions <= .5 & state %% 3 != 0 ~ 1,
actions <= .5 & state %% 3 == 0 ~ -1
)
# Update State
state <- sample(1:1000, 1)
# Terminal state after 100 steps
terminal <- step_index < 5
return(list(state = state, reward = reward, terminal = terminal))
}
reset <- function(){
list(
state = 0 # Initial state of the next episode
)
}
env_train <- py$gymr(init = "init", # Name of init_functin in the global_environment
step = "step", # Name of step_functin in the global_environment
reset = "reset") # Name of reset_functin in the global_environment
sb <- import("stable_baselines3")
model_a2c <- sb$A2C('MlpPolicy', env_train, verbose=0)
model_a2c = model_a2c$learn(total_timesteps = 10000L)
sapply(1:1000, FUN = function(.x) model_a2c$predict(as.matrix(.x), deterministic = T))
reticulate::repl_python()
